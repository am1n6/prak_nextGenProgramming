{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1:** install required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q accelerate==0.34.2 peft==0.6.2 bitsandbytes transformers trl==0.9.6 torch datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2:** Clone repository required for conversion to gguf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3:** Merge the Lora Adapters with the base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./codellama7b_finetuning/trained_model\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Hugging Face token and model details\n",
    "hf_token = \n",
    "MODEL_NAME = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "# Load the tokenizer for the base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load adapter model\n",
    "adapter_model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"./resources/trained_model_adapt_param\",\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "\n",
    "# Convert adapter weights to full precision\n",
    "for name, param in adapter_model.named_parameters():\n",
    "    param.data = param.data.float()  # Convert back to full precision\n",
    "\n",
    "# Merge the adapter weights into the base model\n",
    "newmodel = adapter_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model and tokenizer\n",
    "output_dir = \"./resources/trained_model\"\n",
    "newmodel.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 4:** install required Dependencies for Model file Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: numpy~=1.26.4 in ./.venv/lib/python3.12/site-packages (from -r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: sentencepiece~=0.2.0 in ./.venv/lib/python3.12/site-packages (from -r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.45.1 in ./.venv/lib/python3.12/site-packages (from -r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (4.48.0)\n",
      "Requirement already satisfied: gguf>=0.1.0 in ./.venv/lib/python3.12/site-packages (from -r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in ./.venv/lib/python3.12/site-packages (from -r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.5)\n",
      "Requirement already satisfied: torch~=2.2.1 in ./.venv/lib/python3.12/site-packages (from -r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2+cpu)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (0.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch~=2.2.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->torch~=2.2.1->-r /home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_code_llama_7b/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 5:** Convert Model to GGUF Format\n",
    "\n",
    "NB: if you encounter the problem \"chat_template is duplicated\", just delete \"chat_template\" entry from tokenizer.config in './resources/trained_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: trained_model\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> Q8_0, shape = {4096, 32016}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00005-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00006-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.float32 --> Q8_0, shape = {4096, 32016}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float32 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float32 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float32 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 16384\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 11008\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 32\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 1\n",
      "INFO:gguf.vocab:Setting special token type eos to 2\n",
      "INFO:gguf.vocab:Setting special token type unk to 0\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "WARNING:gguf.vocab:No handler for special token type prefix with id 32007 - skipping\n",
      "WARNING:gguf.vocab:No handler for special token type suffix with id 32008 - skipping\n",
      "WARNING:gguf.vocab:No handler for special token type middle with id 32009 - skipping\n",
      "INFO:gguf.vocab:Setting special token type eot to 32010\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:codellama7b_finetuning/ggufModelFormat/unitTUMcodeLlamaV1.gguf: n_tensors = 291, total_size = 7.2G\n",
      "Writing: 100%|███████████████████████████| 7.16G/7.16G [00:45<00:00, 158Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to codellama7b_finetuning/ggufModelFormat/unitTUMcodeLlamaV1.gguf\n"
     ]
    }
   ],
   "source": [
    "# Convert Hugging Face model to GGUF format\n",
    "!python llama.cpp/convert_hf_to_gguf.py ./resources/trained_model --outfile ./resources/ggufModelFormat/unitTUMcodeLlamaV1.gguf --outtype q8_0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
