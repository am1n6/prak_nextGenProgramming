{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1:** install required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q accelerate==0.34.2 peft==0.6.2 bitsandbytes transformers trl==0.9.6 torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from tensorboardX) (1.26.4)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from tensorboardX) (24.2)\n",
      "Collecting protobuf>=3.20 (from tensorboardX)\n",
      "  Using cached protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Using cached protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Installing collected packages: protobuf, tensorboardX\n",
      "Successfully installed protobuf-5.29.3 tensorboardX-2.6.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2:** import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig, Trainer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3:** define the model name and the dataset used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"tiiuae/falcon-7b-instruct\"\n",
    "DATASET_NAME = \"jitx/Methods2Test_java_unit_test_code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 4:** Load the dataset from remote repository.\n",
    "\n",
    "\n",
    "*   \"train\" section is used for training\n",
    "*   \"test\" section is used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = load_dataset(DATASET_NAME, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = load_dataset(DATASET_NAME, split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 5:** select the partion of datasets used in training and convert it to the model prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for deterministic sorting\n",
    "seed = 42\n",
    "part = 0.02\n",
    "\n",
    "train_partion = training_dataset.shuffle(seed=seed).select(range(int(len(training_dataset) * part)))\n",
    "evaluation_partion = evaluation_dataset.shuffle(seed=seed).select(range(int(len(evaluation_dataset) * part)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted dataset Training saved to ./falcon7b_finetuning/dataset/falcon_format_dataset_train.json\n",
      "{'text': '### System: Generate unit tests for the following method or function:\\n### Human: public Set<String> getOutputResourceFields( T meta ) { return null; }### Assistant: @Test public void testGetOutputResourceFields() throws Exception { assertNull( analyzer.getOutputResourceFields( meta ) ); }'}\n",
      "{'text': '### System: Generate unit tests for the following method or function:\\n### Human: @Override public Long queryFrom(MonetaryAmount amount) { Objects.requireNonNull(amount, \"Amount required.\"); return amount.with(downRounding).getNumber().longValueExact(); }### Assistant: @Test public void shouldReturnMajorPartNegative() { MonetaryAmount monetaryAmount = Money.parse(\"BHD -1.345\"); Long result = query.queryFrom(monetaryAmount); Long expected = -1L; assertEquals(result, expected ); }'}\n",
      "{'text': '### System: Generate unit tests for the following method or function:\\n### Human: public static String wrapWithColorTag(final String str, final Color color) { return prependColorTag(str, color) + CLOSING_COLOR_TAG; }### Assistant: @Test public void wrapWithColorTag() { COLOR_HEXSTRING_MAP.forEach((color, hex) -> { assertEquals(\"<col=\" + hex + \">test</col>\", ColorUtil.wrapWithColorTag(\"test\", color)); assertEquals(\"<col=\" + hex + \"></col>\", ColorUtil.wrapWithColorTag(\"\", color)); }); }'}\n",
      "{'text': '### System: Generate unit tests for the following method or function:\\n### Human: public static Integer getConfigInteger(Map<String, Object> taskConfig, String propertyName, boolean isMandatory) throws TaskConfigurationException { if (taskConfig == null) throw new TaskConfigurationException(propertyName + \" configuration property must be defined\"); Object val = taskConfig.get(propertyName); if (val instanceof Integer) { return (Integer) val; } boolean isEmpty = val == null || val.toString().trim().isEmpty(); if (isMandatory && isEmpty) { throw new TaskConfigurationException(propertyName + \" configuration property must be defined\"); } if (!isMandatory && isEmpty) { return null; } try { return new Integer(val.toString().trim()); } catch (NumberFormatException e) { throw new TaskConfigurationException(propertyName + \" configuration property must be a number\"); } }### Assistant: @Test public void testGetConfigInteger() throws TaskConfigurationException { String PROP_NAME = \"testproperty\"; Map<String, Object> taskConfig = new HashMap<>(); Assert.assertNull(ReindexingTaskFactory.getConfigInteger(taskConfig, PROP_NAME, false)); taskConfig.put(PROP_NAME, new Integer(1)); Assert.assertEquals(new Integer(1), ReindexingTaskFactory.getConfigInteger(taskConfig, PROP_NAME, false)); taskConfig.put(PROP_NAME, \"1\"); Assert.assertEquals(new Integer(1), ReindexingTaskFactory.getConfigInteger(taskConfig, PROP_NAME, false)); }'}\n",
      "{'text': '### System: Generate unit tests for the following method or function:\\n### Human: public static String getBaseFileName(final String fileName, final String suffix) { final int suffixIndex = fileName.lastIndexOf(\".\" + suffix); return (suffixIndex > 0 ? fileName.substring(0, suffixIndex) : fileName); }### Assistant: @Test public void getBaseFileNameComplexExtension() { when( clientResourceType.getSuffix() ).thenReturn( \"suffix.xml\" ); String result = Utils.getBaseFileName( \"filename.suffix.xml\", clientResourceType.getSuffix() ); assertEquals( \"filename\", result ); }'}\n",
      "Converted dataset Evaluation saved to ./falcon7b_finetuning/dataset/falcon_format_dataset_eval.json\n",
      "{'text': '### System: Generate unit tests for the following method or function:\\n### Human: public void postModule(final Module module, final String user, final String password) throws GrapesCommunicationException, AuthenticationException { final Client client = getClient(user, password); final WebResource resource = client.resource(serverURL).path(RequestUtils.moduleResourcePath()); final ClientResponse response = resource.type(MediaType.APPLICATION_JSON).post(ClientResponse.class, module); client.destroy(); if(ClientResponse.Status.CREATED.getStatusCode() != response.getStatus()){ final String message = \"Failed to POST module\"; if(LOG.isErrorEnabled()) { LOG.error(String.format(HTTP_STATUS_TEMPLATE_MSG, message, response.getStatus())); } throw new GrapesCommunicationException(message, response.getStatus()); } }### Assistant: @Test public void postModuleFailed() throws AuthenticationException{ Module module1 = DataModelFactory.createModule(\"module\", \"1.0.0-SNAPSHOT\"); Artifact artifact1 = DataModelFactory.createArtifact(\"com.axway.test\", \"artifact1\", \"1.0.0-SNAPSHOT\", \"win32\", \"jar\", \"\"); Artifact artifact2 = DataModelFactory.createArtifact(\"com.axway.test\", \"artifact2\", \"1.0.0-SNAPSHOT\", \"win32\", \"jar\", \"\"); Artifact dependency = DataModelFactory.createArtifact(\"com.axway.test\", \"dependency\", \"1.0.0-SNAPSHOT\", \"win32\", \"jar\", \"\"); module1.addDependency(DataModelFactory.createDependency(artifact2,Scope.COMPILE)); module1.addDependency(DataModelFactory.createDependency(dependency, Scope.TEST)); module1.addArtifact(artifact1); module1.addArtifact(artifact2); stubFor(post(urlEqualTo(\"/\" + ServerAPI.MODULE_RESOURCE)) .willReturn(aResponse() .withStatus(Status.NOT_ACCEPTABLE.getStatusCode()))); GrapesCommunicationException exception = null; try{ client.postModule(module1, \"user\", \"password\"); }catch (GrapesCommunicationException e) { exception = e; } assertNotNull(exception); assertEquals(Status.NOT_ACCEPTABLE.getStatusCode(), exception.getHttpStatus()); }'}\n",
      "{'text': '### System: Generate unit tests for the following method or function:\\n### Human: @Override public void configure(Settings.Builder settings) { credentials.applyTo(settings); certInfo.applyTo(settings); }### Assistant: @Test public void appliesCredentialsIfConfigured() { Credentials<Settings.Builder> credentials = Mockito.mock(Credentials.class); Settings.Builder settingsBuilder = Settings.builder(); XPackAuth xPackAuth = createTestBuilder() .withCredentials(credentials) .build(); xPackAuth.configure(settingsBuilder); Mockito.verify(credentials).applyTo(builderArgumentCaptor.capture()); Assert.assertEquals(settingsBuilder, builderArgumentCaptor.getValue()); }'}\n",
      "{'text': '### System: Generate unit tests for the following method or function:\\n### Human: @Override public MessageSender<Event<T>> newBackend(T target) { return new EventToDynamicListener<>(target); }### Assistant: @Test public void event_objects_are_serializable() throws Exception { DummyListener target = mock(DummyListener.class); MessageSender<Event<DummyListener>> backend = eventizer.newBackend(target); frontend.onSomething(\"param\"); Event<DummyListener> original = queue.poll(); Event<DummyListener> deserialized = deserialize(serialize(original)); backend.send(deserialized); verify(target).onSomething(\"param\"); }'}\n",
      "{'text': '### System: Generate unit tests for the following method or function:\\n### Human: public String unseal(final String sealedMessage) throws GeneralSecurityException { try { if (protocolVersion.equals(PaymentMethodTokenConstants.PROTOCOL_VERSION_EC_V1)) { return unsealECV1(sealedMessage); } else if (protocolVersion.equals(PaymentMethodTokenConstants.PROTOCOL_VERSION_EC_V2)) { return unsealECV2(sealedMessage); } else if (protocolVersion.equals( PaymentMethodTokenConstants.PROTOCOL_VERSION_EC_V2_SIGNING_ONLY)) { return unsealECV2SigningOnly(sealedMessage); } throw new IllegalArgumentException(\"unsupported version: \" + protocolVersion); } catch (JSONException e) { throw new GeneralSecurityException(\"cannot unseal; invalid JSON message\"); } }### Assistant: @Test public void testShouldFailIfIntermediateSigningKeyIsExpiredInECV2SigningOnly() throws Exception { PaymentMethodTokenRecipient recipient = new PaymentMethodTokenRecipient.Builder() .protocolVersion(PaymentMethodTokenConstants.PROTOCOL_VERSION_EC_V2_SIGNING_ONLY) .senderVerifyingKeys(GOOGLE_VERIFYING_PUBLIC_KEYS_JSON) .recipientId(RECIPIENT_ID) .build(); PaymentMethodTokenSender sender = new PaymentMethodTokenSender.Builder() .protocolVersion(PaymentMethodTokenConstants.PROTOCOL_VERSION_EC_V2_SIGNING_ONLY) .senderIntermediateSigningKey( GOOGLE_SIGNING_EC_V2_SIGNING_ONLY_INTERMEDIATE_PRIVATE_KEY_PKCS8_BASE64) .senderIntermediateCert( new SenderIntermediateCertFactory.Builder() .protocolVersion( PaymentMethodTokenConstants.PROTOCOL_VERSION_EC_V2_SIGNING_ONLY) .addSenderSigningKey(GOOGLE_SIGNING_EC_V2_SIGNING_ONLY_PRIVATE_KEY_PKCS8_BASE64) .senderIntermediateSigningKey( GOOGLE_SIGNING_EC_V2_SIGNING_ONLY_INTERMEDIATE_PUBLIC_KEY_X509_BASE64) .expiration(Instant.now().minus(Duration.standardDays(1)).getMillis()) .build() .create()) .recipientId(RECIPIENT_ID) .build(); try { recipient.unseal(sender.seal(PLAINTEXT)); fail(\"Expected GeneralSecurityException\"); } catch (GeneralSecurityException e) { assertEquals(\"expired intermediateSigningKey\", e.getMessage()); } }'}\n",
      "{'text': '### System: Generate unit tests for the following method or function:\\n### Human: @Override public void editSchedule() { CSQueue root = scheduler.getRootQueue(); Resource clusterResources = Resources.clone(scheduler.getClusterResource()); clusterResources = getNonLabeledResources(clusterResources); setNodeLabels(scheduler.getRMContext().getNodeLabelManager() .getNodeLabels()); containerBasedPreemptOrKill(root, clusterResources); }### Assistant: @Test public void testPreemptCycle() { int[][] qData = new int[][]{ { 100, 40, 40, 20 }, { 100, 100, 100, 100 }, { 100, 0, 60, 40 }, { 10, 10, 0, 0 }, { 0, 0, 0, 0 }, { 3, 1, 1, 1 }, { -1, 1, 1, 1 }, { 3, 0, 0, 0 }, }; ProportionalCapacityPreemptionPolicy policy = buildPolicy(qData); policy.editSchedule(); verify(mDisp, times(10)).handle(argThat(new IsPreemptionRequestFor(appC))); }'}\n"
     ]
    }
   ],
   "source": [
    "INPUT_FIELD = \"src_fm\"\n",
    "OUTPUT_FIELD = \"target\"\n",
    "\n",
    "# Function to convert each example\n",
    "def convert_to_falcon_format(focal_method, target_test_case):\n",
    "    # Define the system prompt\n",
    "    system_prompt = \"Generate unit tests for the following method or function:\\n\"\n",
    "\n",
    "    # Format the example into LLaMA format\n",
    "    formatted_example = f\"### System: {system_prompt}### Human: {focal_method}### Assistant: {target_test_case}\"\n",
    "\n",
    "    return formatted_example\n",
    "\n",
    "# Convert the entire dataset\n",
    "converted_data = [{\"text\": convert_to_falcon_format(entry[INPUT_FIELD], entry[OUTPUT_FIELD])} for entry in train_partion]\n",
    "\n",
    "# Save the converted data to a JSON file\n",
    "output_file = './resources/dataset/falcon_format_dataset_train.json'\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(converted_data, f, indent=4)\n",
    "\n",
    "# Print a few examples to verify the result\n",
    "print(f\"Converted dataset Training saved to {output_file}\")\n",
    "for example in converted_data[:5]:\n",
    "    print(example)\n",
    "\n",
    "# Convert the entire dataset\n",
    "converted_data = [{\"text\" : convert_to_falcon_format(entry[INPUT_FIELD], entry[OUTPUT_FIELD])} for entry in evaluation_partion]\n",
    "\n",
    "# Save the converted data to a JSON file\n",
    "output_file = './resources/dataset/falcon_format_dataset_eval.json'\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(converted_data, f, indent=4)\n",
    "\n",
    "# Print a few examples to verify the result\n",
    "print(f\"Converted dataset Evaluation saved to {output_file}\")\n",
    "for example in converted_data[:5]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface Token required for remote access.\n",
    "# Please update it with your actual Read Token\n",
    "hf_token = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:823: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, use_auth_token=hf_token)\n",
    "\n",
    "# Configure Padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.50s/it]\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    }
   ],
   "source": [
    "# Configure quantization\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_safetensors=True,\n",
    "    quantization_config=quant_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda:0\",\n",
    "    use_auth_token=hf_token\n",
    ")\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n",
       " '_load_in_8bit': False,\n",
       " '_load_in_4bit': True,\n",
       " 'llm_int8_threshold': 6.0,\n",
       " 'llm_int8_skip_modules': None,\n",
       " 'llm_int8_enable_fp32_cpu_offload': False,\n",
       " 'llm_int8_has_fp16_weight': False,\n",
       " 'bnb_4bit_quant_type': 'nf4',\n",
       " 'bnb_4bit_use_double_quant': True,\n",
       " 'bnb_4bit_compute_dtype': 'float16',\n",
       " 'bnb_4bit_quant_storage': 'uint8',\n",
       " 'load_in_4bit': True,\n",
       " 'load_in_8bit': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list quantisation Params of the model for verification\n",
    "model.config.quantization_config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 7:** Train the model with the saved dataset files, then save the new Lora Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Lora Params\n",
    "lora_alpha = 32\n",
    "Lora_dropout = 0.1\n",
    "lora_r = 16\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=Lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_dataset = load_dataset(\"json\", data_files={\"train\" : \"./resources/dataset/falcon_format_dataset_train.json\"})\n",
    "\n",
    "evaluation_dataset = load_dataset(\"json\", data_files={\"validation\" : \"./resources/dataset/falcon_format_dataset_eval.json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=624,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./falcon7b_finetuning/tensorboard/runs/Jan14_13-08-11_amine-ben-abda-Victus-by-HP-Gaming-Laptop-16-s0xxx,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=624,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.COSINE,\n",
      "max_grad_norm=0.3,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2,\n",
      "optim=OptimizerNames.PAGED_ADAMW,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./falcon7b_finetuning/tensorboard,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=falcon7b_finetuning,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=624,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Dataset size and batch parameters\n",
    "dataset_size = 12480\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 2\n",
    "num_gpus = 1\n",
    "\n",
    "# Effective batch size and steps per epoch\n",
    "effective_batch_size = per_device_train_batch_size * gradient_accumulation_steps * num_gpus\n",
    "steps_per_epoch = dataset_size // effective_batch_size\n",
    "\n",
    "# TrainingArguments configuration\n",
    "training_args = TrainingArguments(\n",
    "    # Batch and gradient parameters\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_train_batch_size * 2,  # Double the train batch size for evaluation\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "\n",
    "    # Optimizer and learning rate scheduler\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    # Training duration\n",
    "    num_train_epochs=2,\n",
    "\n",
    "    # Evaluation and logging strategies\n",
    "    evaluation_strategy=\"steps\",  # Evaluate at regular steps\n",
    "    eval_steps=steps_per_epoch // 5,  # Evaluate 5 times per epoch\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=steps_per_epoch // 5,  # Log metrics at the same frequency as evaluation\n",
    "\n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",  # Save checkpoints regularly\n",
    "    save_steps=steps_per_epoch // 5,  # Save 5 times per epoch\n",
    "\n",
    "    # Output and reporting\n",
    "    output_dir=\"./resources/tensorboard\",\n",
    "    report_to=\"tensorboard\",  # Log metrics to TensorBoard\n",
    "    run_name=\"falcon7b_finetuning\",  # Experiment name for tracking\n",
    "\n",
    "    # Miscellaneous\n",
    "    group_by_length=True,  # Group sequences of similar lengths for efficiency\n",
    "    gradient_checkpointing=True,  # Reduce memory usage during training\n",
    "    seed=42,  # Ensures reproducibility\n",
    "    save_total_limit=2,  # Retain only the last 2 checkpoints to save storage\n",
    "    save_safetensors=True,  # Save model checkpoints in a more secure format\n",
    ")\n",
    "\n",
    "# Print the configuration for verification\n",
    "print(training_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:413: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Trainer Initialization\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                        # Model to train\n",
    "    args=training_args,                 # Training arguments\n",
    "    train_dataset=train_dataset[\"train\"],\n",
    "    eval_dataset=evaluation_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6240' max='6240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6240/6240 9:27:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>1.286000</td>\n",
       "      <td>1.179220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1248</td>\n",
       "      <td>1.167900</td>\n",
       "      <td>1.161336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1872</td>\n",
       "      <td>1.162100</td>\n",
       "      <td>1.149861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2496</td>\n",
       "      <td>1.153000</td>\n",
       "      <td>1.143124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>1.141500</td>\n",
       "      <td>1.136528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3744</td>\n",
       "      <td>1.093700</td>\n",
       "      <td>1.134776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4368</td>\n",
       "      <td>1.099700</td>\n",
       "      <td>1.131975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4992</td>\n",
       "      <td>1.083400</td>\n",
       "      <td>1.129775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5616</td>\n",
       "      <td>1.104300</td>\n",
       "      <td>1.128822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6240</td>\n",
       "      <td>1.083400</td>\n",
       "      <td>1.128627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/amine-ben-abda/TUM/WS_2425/Praktikum/finetuning_falcon_7b/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# save the adapter\n",
    "trainer.save_model(output_dir=\"./resources/trained_model_adapt_param\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
